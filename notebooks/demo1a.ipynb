{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter Notebook for performing ETL on the Chicago Taxi Dataset and training a modl to predict whether customers will pay with cash or credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "# !pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially query the dataset to view all the fields and decide which fields are useful and what kind of prediction we can make.\n",
    "\n",
    "Of the 23 fields, we decided to cut the dataset down significantly since many fields did not have complete data, and others share the same information (i.e. community area and the latitude/longitude fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bq_helper\n",
    "from bq_helper import BigQueryHelper\n",
    "\n",
    "\n",
    "#Displays a table with all the labels\n",
    "chicago_taxi = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\", dataset_name=\"chicago_taxi_trips\")\n",
    "bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"chicago_taxi_trips\")\n",
    "bq_assistant.list_tables()\n",
    "bq_assistant.head(\"taxi_trips\", num_rows=3)\n",
    "bq_assistant.table_schema(\"taxi_trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding which fields were useful and the use-case of our model, we ran the query to collect and pre-processed the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project='ml-sandbox-1-191918')\n",
    "\n",
    "\n",
    "dataset_id = 'chicagotaxi'\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "client.delete_table('ml-sandbox-1-191918.chicagotaxi.chicago_taxi_processed', not_found_ok=True)\n",
    "table_ref = client.dataset(dataset_id).table('chicago_taxi_processed')\n",
    "job_config.destination = table_ref\n",
    "\n",
    "\n",
    "query = '''SELECT\n",
    "  IF(payment_type='Cash',1,0) cash,\n",
    "  EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS day_of_week,\n",
    "  (((EXTRACT(HOUR from trip_start_timestamp)*3600)+(EXTRACT(MINUTE from trip_start_timestamp)*60)+(EXTRACT(SECOND from trip_start_timestamp)))/86400) as start_time,\n",
    "  (((EXTRACT(HOUR from trip_end_timestamp)*3600)+(EXTRACT(MINUTE from trip_end_timestamp)*60)+(EXTRACT(SECOND from trip_end_timestamp)))/86400) as end_time,\n",
    "  EXTRACT(DAYOFYEAR FROM trip_start_timestamp) as day_of_year,\n",
    "  EXTRACT(MONTH FROM trip_start_timestamp) as month,\n",
    "  EXTRACT(YEAR FROM trip_start_timestamp) as year,\n",
    "  trip_miles,\n",
    "  (pickup_latitude - 41.660136051)/(42.021223593 - 41.660136051) AS standard_pickup_lat,\n",
    "  (pickup_longitude + 87.913624596)/(-87.531386257 + 87.913624596) AS standard_pickup_long,\n",
    "  (dropoff_latitude - 41.650221676)/(42.021223593 - 41.650221676 ) AS standard_dropoff_lat,\n",
    "  (dropoff_longitude + 87.913624596)/(-87.531386257 + 87.913624596) AS standard_dropoff_long\n",
    "FROM\n",
    "  `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "  trip_miles > 0\n",
    "  AND trip_seconds > 0\n",
    "  AND fare > 0\n",
    "  AND payment_type in ('Cash', 'Credit Card')\n",
    "  AND trip_start_timestamp IS NOT NULL\n",
    "  AND trip_end_timestamp IS NOT NULL\n",
    "  AND trip_miles IS NOT NULL\n",
    "  AND pickup_latitude IS NOT NULL\n",
    "  AND pickup_longitude IS NOT NULL\n",
    "  AND dropoff_latitude IS NOT NULL\n",
    "  AND dropoff_longitude IS NOT NULL;\n",
    "'''\n",
    "\n",
    "query_job = client.query(query, location='US', job_config=job_config)\n",
    "\n",
    "query_job.result()  # Waits for the query to finish\n",
    "print('Query results loaded to table {}'.format(table_ref.path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickup and dropoff latitudes and longitudes were normalized using $\\frac{x-x_{min}}{x_{max}-x_{min}}$. These minimum and maximum values were found using the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  MIN(pickup_latitude) as min_pick_lat,\n",
    "  MAX(pickup_latitude) as max_pick_lat,\n",
    "  MIN(pickup_longitude) as min_pick_lon,\n",
    "  MAX(pickup_longitude) as max_pick_lon,\n",
    "  MIN(dropoff_latitude) as min_drop_lat,\n",
    "  MAX(dropoff_latitude) as max_drop_lat,\n",
    "  MIN(dropoff_longitude) as min_drop_lon,\n",
    "  MAX(dropoff_longitude) as max_drop_lon\n",
    "FROM\n",
    "  `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "  trip_miles > 0\n",
    "  AND trip_seconds > 0\n",
    "  AND fare > 0\n",
    "  AND payment_type in ('Cash', 'Credit Card')\n",
    "  AND trip_start_timestamp IS NOT NULL\n",
    "  AND trip_end_timestamp IS NOT NULL\n",
    "  AND trip_miles IS NOT NULL\n",
    "  AND pickup_latitude IS NOT NULL\n",
    "  AND pickup_longitude IS NOT NULL\n",
    "  AND dropoff_latitude IS NOT NULL\n",
    "  AND dropoff_longitude IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing the data, we ran matrix correlation to check if our problem could be solved simply. We found that there was no direct correlation between payment type and any of the other fields, so we moved on to using a Linear ML Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  CORR(cash,\n",
    "    trip_miles) AS trip_miles_corr,\n",
    "  CORR(cash,\n",
    "    standard_pickup_lat) AS pickup_latitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_pickup_long) AS pickup_longitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_dropoff_lat) AS dropoff_latitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_dropoff_long) AS dropoff_longitude_corr,\n",
    "  CORR(cash,\n",
    "    start_time) AS dropoff_time_corr,\n",
    "  CORR(cash,\n",
    "    year) AS dropoff_year_corr,\n",
    "  CORR(cash,\n",
    "    month) AS month_corr,\n",
    "  CORR(cash,\n",
    "    day_of_year) AS day_corr,\n",
    "  CORR(cash,\n",
    "    day_of_week) AS weekday_corr\n",
    "FROM\n",
    "  `ml-sandbox-1-191918.chicagotaxi.chicago_taxi_processed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then send the processed BigQuery table to a Google Cloud Storage bucket, where it can be accessed by our model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "bucket_name = 'chicago-taxi-data-processed'\n",
    "project = 'ml-sandbox-1-191918'\n",
    "dataset_id = 'chicagotaxi'\n",
    "table_id = 'final_taxi_standardized'\n",
    "destination_uri = 'gs://{}/{}'.format(bucket_name, 'chicago-taxi-*.csv')\n",
    "dataset_ref = client.dataset(dataset_id, project=project)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.job.ExtractJobConfig(print_header=False)\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location='US',\n",
    "    job_config=job_config)  # API request\n",
    "\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "print('Exported {}:{}.{} to {}'.format(\n",
    "    project, dataset_id, table_id, destination_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine files into one for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil compose gs://gcp-cert-demo-1/data/csv/train/*.csv gs://gcp-cert-demo-1/data/csv/train-single.csv!gsutil compose gs://gcp-cert-demo-1/data/csv/test/*.csv gs://gcp-cert-demo-1/data/csv/test-single.csv!gsutil compose gs://gcp-cert-demo-1/data/csv/validate/*.csv gs://gcp-cert-demo-1/data/csv/validate-single.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: large_model_v100\n",
    "  args:\n",
    "    - \"--preprocess\"\n",
    "    - \"--training_data_path=gs://chicago-taxi-data-processed/processed-chicago-taxi.csv\"\n",
    "    - \"--validation_split=0.2\"\n",
    "    - \"--test_split=0.1\"\n",
    "    - \"--job-dir=gs://chicago-taxi-data-processed/tuesday_taxi_2\"\n",
    "    - \"--model_type=classification\"\n",
    "    - \"--max_steps=10000000\"\n",
    "    - \"--learning_rate=0.0002\"\n",
    "    - \"--eval_steps=1000\"\n",
    "    - \"--batch_size=10\"\n",
    "    - \"--eval_frequency_secs=100\"\n",
    "    - \"--optimizer_type=ftrl\"\n",
    "  region: us-east1\n",
    "  jobDir: gs://chicago-taxi-data-processed/\n",
    "  masterConfig:\n",
    "    imageUri: gcr.io/cloud-ml-algos/linear_learner_gpu:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "BUCKET_NAME='chicago-taxi-data-processed'\n",
    "IMAGE_URI='gcr.io/cloud-ml-algos/linear_learner_cpu:latest'\n",
    "\n",
    "# Specify the Cloud Storage path to your training input data.\n",
    "TRAINING_DATA='gs://chicago-taxi-data-processed/processed-chicago-taxi.csv'\n",
    "\n",
    "MODEL_TYPE='classification'\n",
    "JOB_ID = \"tuesday_taxi_{}\".format(int(time.time()))\n",
    "\n",
    "JOB_DIR=\"gs://chicago-taxi-data-processed/algorithm_training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai-platform jobs submit training $JOB_ID \\\n",
    "  --master-image-uri=$IMAGE_URI --config $CONFIG --job-dir=$JOB_DIR --region us-central1\\\n",
    "  -- \\\n",
    "  --preprocess --model_type=$MODEL_TYPE --batch_size=4 \\\n",
    "  --learning_rate=0.001 --max_steps=1000 \\\n",
    "  --training_data_path=$TRAINING_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_NAME=\"taxi_version_{}\".format(int(time.time()))\n",
    "MODEL_NAME=\"taxi_model_{}\".format(int(time.time()))\n",
    "MODEL_DIR=\"gs://chicago-taxi-data-processed/algorithm_training/model\"\n",
    "FRAMEWORK=\"TENSORFLOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine models create $MODEL_NAME --regions us-east1\n",
    "!gcloud ai-platform versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$MODEL_DIR \\\n",
    "  --runtime-version=1.14 \\\n",
    "  --framework $FRAMEWORK \\\n",
    "  --python-version=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "{\"csv_row\":\"6,0.927083333,0.9375,116,4,2013,0.1,0.606309877,0.671360099,0.653137315,0.663879711\",\"key\" : \"dummy-key\"}\n",
    "{\"csv_row\":\"7,0.479166667,0.479166667,82,3,2013,0.1,0.770603177,0.719576369,0.751629483,0.699383324\",\"key\" : \"dummy-key\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict --model $MODEL_NAME --version \\\n",
    "  $VERSION_NAME --json-instances $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=path/to/credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcpdemo1.predictor import Predictor\n",
    "\n",
    "project = 'ml-sandbox-1-191918'\n",
    "model = 'taxi_model_1565105801'\n",
    "\n",
    "instances = [{\"csv_row\": \"6,0.927083333,0.9375,116,4,2013,0.1,0.606309877,0.671360099,0.653137315,0.663879711\", \"key\": \"dummy-key\"}]\n",
    "\n",
    "predictor = Predictor(project, model)\n",
    "\n",
    "print(predictor.predict(instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcpdemo1.tune import HPTuner\n",
    "\n",
    "sa_path = '../credentials/ml-sandbox-1-191918-384dcea092ff.json'\n",
    "project_name = 'ml-sandbox-1-191918'\n",
    "bucket_name = 'gcp-cert-demo-1'\n",
    "local_trainer_package_path = '../mlp_trainer'\n",
    "gcs_trainer_package_path = 'hp_tune_test/trainer-0.1.tar.gz'  # do not include bucket name\n",
    "output_path = 'gs://gcp-cert-demo-1/hp_tuning/hp_tuning_results.csv'\n",
    "job_id_prefix = 'gcpdemo1_mlp_tuning'\n",
    "job_dir_prefix = 'gs://gcp-cert-demo-1/hp_tuning'\n",
    "machine_type = 'complex_model_m_gpu' # https://cloud.google.com/ml-engine/docs/machine-types\n",
    "bq_table_id = 'finaltaxi_encoded_sampled_small'\n",
    "\n",
    "# Optimizer parameters:\n",
    "#      \"Adam\"    for tf.keras.optimizers.Adam\n",
    "#      \"Nadam\"   for tf.keras.optimizers.Nadam\n",
    "#      \"RMSprop\" for tf.keras.optimizers.RMSprop\n",
    "#      \"SGD\"     for tf.keras.optimizers.SGD\n",
    "\n",
    "# params = {\n",
    "#     # Tunable params\n",
    "#     \"dense_neurons_1\": [64, 128, 9],\n",
    "#     \"dense_neurons_2\": [32, 64, 5],\n",
    "#     \"dense_neurons_3\": [8, 32, 7],\n",
    "#     \"activation\": [\"relu\", \"elu\"],\n",
    "#     \"dropout_rate_1\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_2\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_3\": [0, 0.5, 5],\n",
    "#     \"optimizer\": [\"Adam\", \"Nadam\", \"RMSprop\", \"SGD\"],\n",
    "#     \"learning_rate\": [.0001, .0005, .001, .005, .01, .05, .1, .5, 1],\n",
    "#     \"kernel_initial_1\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_2\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_3\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "\n",
    "#     # Static params\n",
    "#     \"batch_size\": [128],\n",
    "#     \"chunk_size\": [500000],\n",
    "#     \"epochs\": [40],\n",
    "#     \"validation_freq\": [1],\n",
    "#     \"patience\": [20]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    \"dense_neurons_1\": [64, 9],\n",
    "    \"dense_neurons_2\": [32],\n",
    "    \"dense_neurons_3\": [8],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"dropout_rate_1\": [0.5],\n",
    "    \"dropout_rate_2\": [0.5],\n",
    "    \"dropout_rate_3\": [0.5],\n",
    "    \"optimizer\": [\"Adam\"],\n",
    "    \"learning_rate\": [.0001],\n",
    "    \"kernel_initial_1\": [\"normal\"],\n",
    "    \"kernel_initial_2\": [\"normal\"],\n",
    "    \"kernel_initial_3\": [\"normal\"],\n",
    "\n",
    "    \"batch_size\": [1024],\n",
    "    \"chunk_size\": [500000],\n",
    "    \"epochs\": [40],\n",
    "    \"validation_freq\": [5],\n",
    "    \"patience\": [5]\n",
    "}\n",
    "\n",
    "hp_tuner = HPTuner(project_name=project_name,\n",
    "                   job_id_prefix=job_id_prefix,\n",
    "                   master_type=machine_type,\n",
    "                   job_dir_prefix=job_dir_prefix,\n",
    "                   table_id=bq_table_id)\n",
    "\n",
    "tuning_log_path = hp_tuner.tune(bucket_name, gcs_trainer_package_path, local_trainer_package_path, params, output_path, sa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 GET https://www.googleapis.com/download/storage/v1/b/gcp-cert-demo-1/o/hp_tune_test%2Fhp_tuning_results.csv?alt=media: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\cloud\\storage\\blob.py\u001b[0m in \u001b[0;36mdownload_to_file\u001b[1;34m(self, file_obj, client, start, end)\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\cloud\\storage\\blob.py\u001b[0m in \u001b[0;36m_do_download\u001b[1;34m(self, transport, file_obj, download_url, headers, start, end)\u001b[0m\n\u001b[0;32m    612\u001b[0m             )\n\u001b[1;32m--> 613\u001b[1;33m             \u001b[0mdownload\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\resumable_media\\requests\\download.py\u001b[0m in \u001b[0;36mconsume\u001b[1;34m(self, transport)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\resumable_media\\_download.py\u001b[0m in \u001b[0;36m_process_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    170\u001b[0m         _helpers.require_status_code(\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ACCEPTABLE_STATUS_CODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_status_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\resumable_media\\_helpers.py\u001b[0m in \u001b[0;36mrequire_status_code\u001b[1;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;34mu\"Expected one of\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[1;33m*\u001b[0m\u001b[0mstatus_codes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         )\n",
      "\u001b[1;31mInvalidResponse\u001b[0m: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c1e72b381e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlocal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tuning_results.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_blob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuning_log_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtuning_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\gcpdemo1\\tune.py\u001b[0m in \u001b[0;36mdownload_blob\u001b[1;34m(bucket_name, source_blob_name, destination_file_name, credentials)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_blob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_to_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\cloud\\storage\\blob.py\u001b[0m in \u001b[0;36mdownload_to_filename\u001b[1;34m(self, filename, client, start, end)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataCorruption\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[1;31m# Delete the corrupt downloaded file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\cloud\\storage\\blob.py\u001b[0m in \u001b[0;36mdownload_to_file\u001b[1;34m(self, file_obj, client, start, end)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mresumable_media\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidResponse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0m_raise_from_invalid_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload_to_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gcp-demo1\\lib\\site-packages\\google\\cloud\\storage\\blob.py\u001b[0m in \u001b[0;36m_raise_from_invalid_response\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m   2087\u001b[0m     )\n\u001b[0;32m   2088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2089\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_http_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 GET https://www.googleapis.com/download/storage/v1/b/gcp-cert-demo-1/o/hp_tune_test%2Fhp_tuning_results.csv?alt=media: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)"
     ]
    }
   ],
   "source": [
    "import gcpdemo1.tune as tune\n",
    "\n",
    "tuning_log_path = 'hp_tune_test/hp_tuning_results.csv'\n",
    "sa_path = '../credentials/ml-sandbox-1-191918-384dcea092ff.json'\n",
    "bucket_name = 'gcp-cert-demo-1'\n",
    "local_path = 'tuning_results.csv'\n",
    "\n",
    "tune.download_blob(bucket_name, tuning_log_path, local_path, sa_path)\n",
    "\n",
    "tuning_output = pd.read_csv(local_path)\n",
    "head(tuning_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call('python ../mlp_trainer/setup.py sdist'.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gcp-demo1]",
   "language": "python",
   "name": "conda-env-gcp-demo1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
