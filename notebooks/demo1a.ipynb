{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter Notebook for performing ETL on the Chicago Taxi Dataset and training a modl to predict whether customers will pay with cash or credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "# !pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially query the dataset to view all the fields and decide which fields are useful and what kind of prediction we can make.\n",
    "\n",
    "Of the 23 fields, we decided to cut the dataset down significantly since many fields did not have complete data, and others share the same information (i.e. community area and the latitude/longitude fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bq_helper\n",
    "from bq_helper import BigQueryHelper\n",
    "\n",
    "\n",
    "#Displays a table with all the labels\n",
    "chicago_taxi = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\", dataset_name=\"chicago_taxi_trips\")\n",
    "bq_assistant = BigQueryHelper(\"bigquery-public-data\", \"chicago_taxi_trips\")\n",
    "bq_assistant.list_tables()\n",
    "bq_assistant.head(\"taxi_trips\", num_rows=3)\n",
    "bq_assistant.table_schema(\"taxi_trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deciding which fields were useful and the use-case of our model, we ran the query to collect and pre-processed the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project='ml-sandbox-1-191918')\n",
    "\n",
    "\n",
    "dataset_id = 'chicagotaxi'\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "client.delete_table('ml-sandbox-1-191918.chicagotaxi.chicago_taxi_processed', not_found_ok=True)\n",
    "table_ref = client.dataset(dataset_id).table('chicago_taxi_processed')\n",
    "job_config.destination = table_ref\n",
    "\n",
    "\n",
    "query = '''SELECT\n",
    "  IF(payment_type='Cash',1,0) cash,\n",
    "  EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS day_of_week,\n",
    "  (((EXTRACT(HOUR from trip_start_timestamp)*3600)+(EXTRACT(MINUTE from trip_start_timestamp)*60)+(EXTRACT(SECOND from trip_start_timestamp)))/86400) as start_time,\n",
    "  (((EXTRACT(HOUR from trip_end_timestamp)*3600)+(EXTRACT(MINUTE from trip_end_timestamp)*60)+(EXTRACT(SECOND from trip_end_timestamp)))/86400) as end_time,\n",
    "  EXTRACT(DAYOFYEAR FROM trip_start_timestamp) as day_of_year,\n",
    "  EXTRACT(MONTH FROM trip_start_timestamp) as month,\n",
    "  EXTRACT(YEAR FROM trip_start_timestamp) as year,\n",
    "  trip_miles,\n",
    "  (pickup_latitude - 41.660136051)/(42.021223593 - 41.660136051) AS standard_pickup_lat,\n",
    "  (pickup_longitude + 87.913624596)/(-87.531386257 + 87.913624596) AS standard_pickup_long,\n",
    "  (dropoff_latitude - 41.650221676)/(42.021223593 - 41.650221676 ) AS standard_dropoff_lat,\n",
    "  (dropoff_longitude + 87.913624596)/(-87.531386257 + 87.913624596) AS standard_dropoff_long\n",
    "FROM\n",
    "  `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "  trip_miles > 0\n",
    "  AND trip_seconds > 0\n",
    "  AND fare > 0\n",
    "  AND payment_type in ('Cash', 'Credit Card')\n",
    "  AND trip_start_timestamp IS NOT NULL\n",
    "  AND trip_end_timestamp IS NOT NULL\n",
    "  AND trip_miles IS NOT NULL\n",
    "  AND pickup_latitude IS NOT NULL\n",
    "  AND pickup_longitude IS NOT NULL\n",
    "  AND dropoff_latitude IS NOT NULL\n",
    "  AND dropoff_longitude IS NOT NULL;\n",
    "'''\n",
    "\n",
    "query_job = client.query(query, location='US', job_config=job_config)\n",
    "\n",
    "query_job.result()  # Waits for the query to finish\n",
    "print('Query results loaded to table {}'.format(table_ref.path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickup and dropoff latitudes and longitudes were normalized using $\\frac{x-x_{min}}{x_{max}-x_{min}}$. These minimum and maximum values were found using the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  MIN(pickup_latitude) as min_pick_lat,\n",
    "  MAX(pickup_latitude) as max_pick_lat,\n",
    "  MIN(pickup_longitude) as min_pick_lon,\n",
    "  MAX(pickup_longitude) as max_pick_lon,\n",
    "  MIN(dropoff_latitude) as min_drop_lat,\n",
    "  MAX(dropoff_latitude) as max_drop_lat,\n",
    "  MIN(dropoff_longitude) as min_drop_lon,\n",
    "  MAX(dropoff_longitude) as max_drop_lon\n",
    "FROM\n",
    "  `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "  trip_miles > 0\n",
    "  AND trip_seconds > 0\n",
    "  AND fare > 0\n",
    "  AND payment_type in ('Cash', 'Credit Card')\n",
    "  AND trip_start_timestamp IS NOT NULL\n",
    "  AND trip_end_timestamp IS NOT NULL\n",
    "  AND trip_miles IS NOT NULL\n",
    "  AND pickup_latitude IS NOT NULL\n",
    "  AND pickup_longitude IS NOT NULL\n",
    "  AND dropoff_latitude IS NOT NULL\n",
    "  AND dropoff_longitude IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing the data, we ran matrix correlation to check if our problem could be solved simply. We found that there was no direct correlation between payment type and any of the other fields, so we moved on to using a Linear ML Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  CORR(cash,\n",
    "    trip_miles) AS trip_miles_corr,\n",
    "  CORR(cash,\n",
    "    standard_pickup_lat) AS pickup_latitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_pickup_long) AS pickup_longitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_dropoff_lat) AS dropoff_latitude_corr,\n",
    "  CORR(cash,\n",
    "    standard_dropoff_long) AS dropoff_longitude_corr,\n",
    "  CORR(cash,\n",
    "    start_time) AS dropoff_time_corr,\n",
    "  CORR(cash,\n",
    "    year) AS dropoff_year_corr,\n",
    "  CORR(cash,\n",
    "    month) AS month_corr,\n",
    "  CORR(cash,\n",
    "    day_of_year) AS day_corr,\n",
    "  CORR(cash,\n",
    "    day_of_week) AS weekday_corr\n",
    "FROM\n",
    "  `ml-sandbox-1-191918.chicagotaxi.chicago_taxi_processed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then send the processed BigQuery table to a Google Cloud Storage bucket, where it can be accessed by our model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "bucket_name = 'chicago-taxi-data-processed'\n",
    "project = 'ml-sandbox-1-191918'\n",
    "dataset_id = 'chicagotaxi'\n",
    "table_id = 'final_taxi_standardized'\n",
    "destination_uri = 'gs://{}/{}'.format(bucket_name, 'chicago-taxi-*.csv')\n",
    "dataset_ref = client.dataset(dataset_id, project=project)\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.job.ExtractJobConfig(print_header=False)\n",
    "\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location='US',\n",
    "    job_config=job_config)  # API request\n",
    "\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "print('Exported {}:{}.{} to {}'.format(\n",
    "    project, dataset_id, table_id, destination_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine files into one for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil compose gs://gcp-cert-demo-1/data/csv/train/*.csv gs://gcp-cert-demo-1/data/csv/train-single.csv!gsutil compose gs://gcp-cert-demo-1/data/csv/test/*.csv gs://gcp-cert-demo-1/data/csv/test-single.csv!gsutil compose gs://gcp-cert-demo-1/data/csv/validate/*.csv gs://gcp-cert-demo-1/data/csv/validate-single.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: large_model_v100\n",
    "  args:\n",
    "    - \"--preprocess\"\n",
    "    - \"--training_data_path=gs://chicago-taxi-data-processed/processed-chicago-taxi.csv\"\n",
    "    - \"--validation_split=0.2\"\n",
    "    - \"--test_split=0.1\"\n",
    "    - \"--job-dir=gs://chicago-taxi-data-processed/tuesday_taxi_2\"\n",
    "    - \"--model_type=classification\"\n",
    "    - \"--max_steps=10000000\"\n",
    "    - \"--learning_rate=0.0002\"\n",
    "    - \"--eval_steps=1000\"\n",
    "    - \"--batch_size=10\"\n",
    "    - \"--eval_frequency_secs=100\"\n",
    "    - \"--optimizer_type=ftrl\"\n",
    "  region: us-east1\n",
    "  jobDir: gs://chicago-taxi-data-processed/\n",
    "  masterConfig:\n",
    "    imageUri: gcr.io/cloud-ml-algos/linear_learner_gpu:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "BUCKET_NAME='chicago-taxi-data-processed'\n",
    "IMAGE_URI='gcr.io/cloud-ml-algos/linear_learner_cpu:latest'\n",
    "\n",
    "# Specify the Cloud Storage path to your training input data.\n",
    "TRAINING_DATA='gs://chicago-taxi-data-processed/processed-chicago-taxi.csv'\n",
    "\n",
    "MODEL_TYPE='classification'\n",
    "JOB_ID = \"tuesday_taxi_{}\".format(int(time.time()))\n",
    "\n",
    "JOB_DIR=\"gs://chicago-taxi-data-processed/algorithm_training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai-platform jobs submit training $JOB_ID \\\n",
    "  --master-image-uri=$IMAGE_URI --config $CONFIG --job-dir=$JOB_DIR --region us-central1\\\n",
    "  -- \\\n",
    "  --preprocess --model_type=$MODEL_TYPE --batch_size=4 \\\n",
    "  --learning_rate=0.001 --max_steps=1000 \\\n",
    "  --training_data_path=$TRAINING_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_NAME=\"taxi_version_{}\".format(int(time.time()))\n",
    "MODEL_NAME=\"taxi_model_{}\".format(int(time.time()))\n",
    "MODEL_DIR=\"gs://chicago-taxi-data-processed/algorithm_training/model\"\n",
    "FRAMEWORK=\"TENSORFLOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine models create $MODEL_NAME --regions us-east1\n",
    "!gcloud ai-platform versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$MODEL_DIR \\\n",
    "  --runtime-version=1.14 \\\n",
    "  --framework $FRAMEWORK \\\n",
    "  --python-version=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "{\"csv_row\":\"6,0.927083333,0.9375,116,4,2013,0.1,0.606309877,0.671360099,0.653137315,0.663879711\",\"key\" : \"dummy-key\"}\n",
    "{\"csv_row\":\"7,0.479166667,0.479166667,82,3,2013,0.1,0.770603177,0.719576369,0.751629483,0.699383324\",\"key\" : \"dummy-key\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict --model $MODEL_NAME --version \\\n",
    "  $VERSION_NAME --json-instances $INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=path/to/credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcpdemo1.predictor import Predictor\n",
    "\n",
    "project = 'ml-sandbox-1-191918'\n",
    "model = 'taxi_model_1565105801'\n",
    "\n",
    "instances = [{\"csv_row\": \"6,0.927083333,0.9375,116,4,2013,0.1,0.606309877,0.671360099,0.653137315,0.663879711\", \"key\": \"dummy-key\"}]\n",
    "\n",
    "predictor = Predictor(project, model)\n",
    "\n",
    "print(predictor.predict(instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcpdemo1.tune import HPTuner\n",
    "\n",
    "sa_path = '../credentials/ml-sandbox-1-191918-384dcea092ff.json'\n",
    "project_name = 'ml-sandbox-1-191918'\n",
    "bucket_name = 'gcp-cert-demo-1'\n",
    "local_trainer_package_path = '../mlp_trainer'\n",
    "gcs_trainer_package_path = 'hp_tune_test/trainer-0.1.tar.gz'  # do not include bucket name\n",
    "output_path = 'gs://gcp-cert-demo-1/hp_tuning/hp_tuning_results.csv'\n",
    "job_id_prefix = 'gcpdemo1_mlp_tuning'\n",
    "job_dir_prefix = 'gs://gcp-cert-demo-1/hp_tuning'\n",
    "machine_type = 'complex_model_m_gpu' # https://cloud.google.com/ml-engine/docs/machine-types\n",
    "bq_table_id = 'finaltaxi_encoded_sampled_small'\n",
    "\n",
    "# Optimizer parameters:\n",
    "#      \"Adam\"    for tf.keras.optimizers.Adam\n",
    "#      \"Nadam\"   for tf.keras.optimizers.Nadam\n",
    "#      \"RMSprop\" for tf.keras.optimizers.RMSprop\n",
    "#      \"SGD\"     for tf.keras.optimizers.SGD\n",
    "\n",
    "# params = {\n",
    "#     # Tunable params\n",
    "#     \"dense_neurons_1\": [64, 128, 9],\n",
    "#     \"dense_neurons_2\": [32, 64, 5],\n",
    "#     \"dense_neurons_3\": [8, 32, 7],\n",
    "#     \"activation\": [\"relu\", \"elu\"],\n",
    "#     \"dropout_rate_1\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_2\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_3\": [0, 0.5, 5],\n",
    "#     \"optimizer\": [\"Adam\", \"Nadam\", \"RMSprop\", \"SGD\"],\n",
    "#     \"learning_rate\": [.0001, .0005, .001, .005, .01, .05, .1, .5, 1],\n",
    "#     \"kernel_initial_1\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_2\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_3\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "\n",
    "#     # Static params\n",
    "#     \"batch_size\": [128],\n",
    "#     \"chunk_size\": [500000],\n",
    "#     \"epochs\": [40],\n",
    "#     \"validation_freq\": [1],\n",
    "#     \"patience\": [20]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    \"dense_neurons_1\": [64, 9],\n",
    "    \"dense_neurons_2\": [32],\n",
    "    \"dense_neurons_3\": [8],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"dropout_rate_1\": [0.5],\n",
    "    \"dropout_rate_2\": [0.5],\n",
    "    \"dropout_rate_3\": [0.5],\n",
    "    \"optimizer\": [\"Adam\"],\n",
    "    \"learning_rate\": [.0001],\n",
    "    \"kernel_initial_1\": [\"normal\"],\n",
    "    \"kernel_initial_2\": [\"normal\"],\n",
    "    \"kernel_initial_3\": [\"normal\"],\n",
    "\n",
    "    \"batch_size\": [1024],\n",
    "    \"chunk_size\": [500000],\n",
    "    \"epochs\": [40],\n",
    "    \"validation_freq\": [5],\n",
    "    \"patience\": [5]\n",
    "}\n",
    "\n",
    "hp_tuner = HPTuner(project_name=project_name,\n",
    "                   job_id_prefix=job_id_prefix,\n",
    "                   master_type=machine_type,\n",
    "                   job_dir_prefix=job_dir_prefix,\n",
    "                   table_id=bq_table_id)\n",
    "\n",
    "tuning_log_path = hp_tuner.tune(bucket_name, gcs_trainer_package_path, local_trainer_package_path, params, output_path, sa_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_f1_metric</th>\n",
       "      <th>f1_metric</th>\n",
       "      <th>acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>...</th>\n",
       "      <th>dropout_rate_2</th>\n",
       "      <th>dropout_rate_3</th>\n",
       "      <th>epochs</th>\n",
       "      <th>kernel_initial_1</th>\n",
       "      <th>kernel_initial_2</th>\n",
       "      <th>kernel_initial_3</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>patience</th>\n",
       "      <th>validation_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.695255</td>\n",
       "      <td>0.717657</td>\n",
       "      <td>0.700835</td>\n",
       "      <td>0.597354</td>\n",
       "      <td>0.559827</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>normal</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>500000</td>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.733860</td>\n",
       "      <td>0.717436</td>\n",
       "      <td>0.698905</td>\n",
       "      <td>0.588882</td>\n",
       "      <td>0.559652</td>\n",
       "      <td>0.667636</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>normal</td>\n",
       "      <td>...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "      <td>500000</td>\n",
       "      <td>8</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  round_epochs  loss  val_f1_metric  f1_metric       acc  \\\n",
       "0           0             9  0.695255       0.717657   0.700835  0.597354   \n",
       "1           1            15  0.733860       0.717436   0.698905  0.588882   \n",
       "\n",
       "    val_acc      loss  activation batch_size  ... dropout_rate_2  \\\n",
       "0  0.559827  0.665019      0.0001     normal  ...         normal   \n",
       "1  0.559652  0.667636      0.0001     normal  ...         normal   \n",
       "\n",
       "  dropout_rate_3  epochs  kernel_initial_1  kernel_initial_2 kernel_initial_3  \\\n",
       "0            0.5       5                32               0.5                5   \n",
       "1            0.5       5                32               0.5                5   \n",
       "\n",
       "                                       learning_rate  optimizer  patience  \\\n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.a...     500000         8   \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.a...     500000         8   \n",
       "\n",
       "   validation_freq  \n",
       "0             1024  \n",
       "1             1024  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcpdemo1.tune as tune\n",
    "import pandas as pd\n",
    "sa_path = '../credentials/ml-sandbox-1-191918-384dcea092ff.json'\n",
    "tuning_log_path = 'hp_tuning/hp_tuning_results.csv'\n",
    "bucket_name = 'gcp-cert-demo-1'\n",
    "local_path = 'tuning_results.csv'\n",
    "\n",
    "tune.download_blob(bucket_name, tuning_log_path, local_path, sa_path)\n",
    "\n",
    "tuning_output = pd.read_csv(local_path)\n",
    "tuning_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call('python ../mlp_trainer/setup.py sdist'.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gcp-demo1]",
   "language": "python",
   "name": "conda-env-gcp-demo1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
