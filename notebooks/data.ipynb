{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import codecs\n",
    "sys.path.append('mlp_trainer')\n",
    "\n",
    "# import GCP Demo 1 package\n",
    "import trainer.data as data\n",
    "import trainer.model as model\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery_storage_v1beta1\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, readers = data.get_data_partition_sharded(\"finaltaxi_encoded_sampled\", \"training\", shards=20)\n",
    "print(session.streams[0].name)\n",
    "sp = pickle.dumps(session)\n",
    "print(pickle.loads(sp))\n",
    "print(readers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    model.generator_input,\n",
    "    (tf.float64, tf.uint16),\n",
    "    output_shapes=(tf.TensorShape([None]), tf.TensorShape([])),\n",
    "    args = (            \"finaltaxi_encoded_sampled_small\",\n",
    "        1,\n",
    "        8,\n",
    "        'train')\n",
    ").batch(2).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for value in dataset.take(1000000):\n",
    "    if count % 100000 == 0:\n",
    "        print(value)\n",
    "    count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:Entity <function <lambda> at 0x7f83dc427560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\nWARNING: Entity <function <lambda> at 0x7f83dc427560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n(<tf.Tensor: id=1247, shape=(2, 26), dtype=float32, numpy=\narray([[ 2.0160000e+03,  4.2708829e-01, -1.4583333e-01,  1.7103280e+00,\n        -8.0152071e-01, -8.4169000e-02,  8.9659065e-02,  0.0000000e+00,\n         1.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00],\n       [ 2.0130000e+03,  6.4584082e-01,  2.9167342e-01, -3.6133602e-02,\n        -1.1567656e-01, -1.5770180e-02,  3.7937798e-02,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         1.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n         0.0000000e+00,  1.0000000e+00]], dtype=float32)>, <tf.Tensor: id=1248, shape=(2,), dtype=uint16, numpy=array([1, 0], dtype=uint16)>)\n"
    }
   ],
   "source": [
    "def get_reader_for_stream(session_pickled: bytes, stream_name_bytes: bytes):\n",
    "    session = pickle.loads(codecs.decode(session_pickled, \"base64\"))\n",
    "    stream_name = stream_name_bytes.decode(\"utf-8\")\n",
    "    client = bigquery_storage_v1beta1.BigQueryStorageClient()\n",
    "    for stream in session.streams:\n",
    "        if stream.name == stream_name:\n",
    "            reader = data.get_reader(client, stream)\n",
    "            rows = reader.rows(session)\n",
    "            for row in rows:\n",
    "                # df = pd.DataFrame([row])\n",
    "                yield (\n",
    "                    # df.drop(['cash', ], axis=1).values,\n",
    "                    # df['cash'].values \n",
    "                    [\n",
    "                        row.get(\"year\"),\n",
    "                        row.get(\"start_time_norm_midnight\"),\n",
    "                        row.get(\"start_time_norm_noon\"),\n",
    "                        row.get(\"pickup_lat_std\"),\n",
    "                        row.get(\"pickup_long_std\"),\n",
    "                        row.get(\"pickup_lat_centered\"),\n",
    "                        row.get(\"pickup_long_centered\"),\n",
    "                        row.get(\"day_of_week_MONDAY\"),\n",
    "                        row.get(\"day_of_week_TUESDAY\"),\n",
    "                        row.get(\"day_of_week_WEDNESDAY\"),\n",
    "                        row.get(\"day_of_week_THURSDAY\"),\n",
    "                        row.get(\"day_of_week_FRIDAY\"),\n",
    "                        row.get(\"day_of_week_SATURDAY\"),\n",
    "                        row.get(\"day_of_week_SUNDAY\"),\n",
    "                        row.get(\"month_JANUARY\"),\n",
    "                        row.get(\"month_FEBRUARY\"),\n",
    "                        row.get(\"month_MARCH\"),\n",
    "                        row.get(\"month_APRIL\"),\n",
    "                        row.get(\"month_MAY\"),\n",
    "                        row.get(\"month_JUNE\"),\n",
    "                        row.get(\"month_JULY\"),\n",
    "                        row.get(\"month_AUGUST\"),\n",
    "                        row.get(\"month_SEPTEMBER\"),\n",
    "                        row.get(\"month_OCTOBER\"),\n",
    "                        row.get(\"month_NOVEMBER\"),\n",
    "                        row.get(\"month_DECEMBER\"),\n",
    "                    ],\n",
    "                    row.get(\"cash\")\n",
    "                )\n",
    "\n",
    "def bq_stream_generator(table_id: bytes, partition: bytes):\n",
    "    session, _ = data.get_data_partition_sharded(table_id.decode(\"utf-8\"), partition.decode(\"utf-8\"), shards=100)\n",
    "    encoded_session = codecs.encode(pickle.dumps(session), \"base64\")\n",
    "    for stream in session.streams:\n",
    "        yield(encoded_session, stream.name)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    bq_stream_generator,\n",
    "    (tf.string, tf.string),\n",
    "    # output_shapes=(tf.TensorShape([]), tf.TensorShape([])),\n",
    "    args = (            \"finaltaxi_encoded_sampled\",\n",
    "        'train')\n",
    ").interleave(lambda session, stream: \n",
    "    tf.data.Dataset.from_generator(\n",
    "        get_reader_for_stream,\n",
    "        (tf.float64, tf.uint16),\n",
    "        args=(session, stream)\n",
    "    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ").prefetch(buffer_size=tf.data.experimental.AUTOTUNE).batch(2).repeat(2)\n",
    "\n",
    "count = 0\n",
    "for value in dataset.take(1):\n",
    "    # if count % 100000 == 0:\n",
    "    print(value)\n",
    "    count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}