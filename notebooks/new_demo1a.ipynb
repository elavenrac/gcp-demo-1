{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import GCP Demo 1 package\n",
    "from gcpdemo1 import gcp, etl, data, tune, train, predict\n",
    "from gcpdemo1.tune import MLPTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***USER INPUT***: Enter GCP Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP variables\n",
    "gcp_vars = {\n",
    "    'credentials_path': '../credentials/ml-sandbox-1-191918-384dcea092ff.json', # local path to gcp credentials\n",
    "    'project_name': 'ml-sandbox-1-191918', # gcp project name\n",
    "    'bucket': 'gcp-cert-demo-1', # gcp bucket name\n",
    "    'gcs_trainer_path': 'taxi_mlp_trainer/trainer-0.1.tar.gz', # path to write trainer package to within bucket\n",
    "    'local_trainer_path': '../mlp_trainer', # local path to trainer package\n",
    "    'table_id': 'finaltaxi_encoded_sampled_small' # bigquery table to use as dataset TODO: temporary small table\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GCP credentials\n",
    "gcp_credentials = gcp.get_credentials(gcp_vars['credentials_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and upload trainer package\n",
    "# TODO: does not work on windows because of no bash command\n",
    "gcp_vars['package_uri'] = gcp.build_and_upload_trainer_package(\n",
    "    bucket_name=gcp_vars['bucket'],\n",
    "    destination_blob_name=gcp_vars['gcs_trainer_path'],\n",
    "    local_trainer_package_path=gcp_vars['local_trainer_path'],\n",
    "    credentials=gcp_credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_prefix = 'gcpdemo1_mlp_tuning'\n",
    "job_dir_prefix = 'gs://gcp-cert-demo-1/hp_tuning'\n",
    "output_path = 'gs://gcp-cert-demo-1/hp_tuning/hp_tuning_results.csv'\n",
    "machine_type = 'complex_model_m_gpu' # https://cloud.google.com/ml-engine/docs/machine-types\n",
    "\n",
    "# Optimizer parameters:\n",
    "#      \"Adam\"    for tf.keras.optimizers.Adam\n",
    "#      \"Nadam\"   for tf.keras.optimizers.Nadam\n",
    "#      \"RMSprop\" for tf.keras.optimizers.RMSprop\n",
    "#      \"SGD\"     for tf.keras.optimizers.SGD\n",
    "\n",
    "# params = {\n",
    "#     # Tunable params\n",
    "#     \"dense_neurons_1\": [64, 128, 9],\n",
    "#     \"dense_neurons_2\": [32, 64, 5],\n",
    "#     \"dense_neurons_3\": [8, 32, 7],\n",
    "#     \"activation\": [\"relu\", \"elu\"],\n",
    "#     \"dropout_rate_1\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_2\": [0, 0.5, 5],\n",
    "#     \"dropout_rate_3\": [0, 0.5, 5],\n",
    "#     \"optimizer\": [\"Adam\", \"Nadam\", \"RMSprop\", \"SGD\"],\n",
    "#     \"learning_rate\": [.0001, .0005, .001, .005, .01, .05, .1, .5, 1],\n",
    "#     \"kernel_initial_1\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_2\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "#     \"kernel_initial_3\": [\"normal\", \"glorot_normal\", \"he_normal\", \"lecun_normal\"],\n",
    "\n",
    "#     # Static params\n",
    "#     \"batch_size\": [128],\n",
    "#     \"chunk_size\": [500000],\n",
    "#     \"epochs\": [40],\n",
    "#     \"validation_freq\": [1],\n",
    "#     \"patience\": [20]\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    \"dense_neurons_1\": [64, 9],\n",
    "    \"dense_neurons_2\": [32],\n",
    "    \"dense_neurons_3\": [8],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"dropout_rate_1\": [0.5],\n",
    "    \"dropout_rate_2\": [0.5],\n",
    "    \"dropout_rate_3\": [0.5],\n",
    "    \"optimizer\": [\"Adam\"],\n",
    "    \"learning_rate\": [.0001],\n",
    "    \"kernel_initial_1\": [\"normal\"],\n",
    "    \"kernel_initial_2\": [\"normal\"],\n",
    "    \"kernel_initial_3\": [\"normal\"],\n",
    "\n",
    "    \"batch_size\": [1024],\n",
    "    \"chunk_size\": [500000],\n",
    "    \"epochs\": [1],\n",
    "    \"validation_freq\": [1],\n",
    "    \"patience\": [5]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mlp tuner\n",
    "mlp_tuner = tune.MLPTuner(project_name=gcp_vars['project_name'],\n",
    "                   credentials=gcp_credentials,\n",
    "                   job_id_prefix=job_id_prefix,\n",
    "                   master_type=machine_type,\n",
    "                   job_dir_prefix=job_dir_prefix,\n",
    "                   table_id=gcp_vars['table_id'])\n",
    "\n",
    "# begin tuning job\n",
    "tuning_log_path = mlp_tuner.tune(gcp_vars['package_uri'], params, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status of tuning job\n",
    "print(f'Tuning job status: {gcp.check_mle_job_status(mlp_tuner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review tuning output (this is now done in GCP - AI Platform)\n",
    "local_results_path = './tuning_results.csv'\n",
    "gcp.download_blob(gcp_vars['bucket'], tuning_log_path, local_results_path, credentials)\n",
    "\n",
    "tuning_output = pd.read_csv(local_path)\n",
    "tuning_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***USER INPUT***: Enter training and job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input job parameters\n",
    "job_id_prefix = 'taxi_mlp_trainer', # training job ID\n",
    "job_dir_prefix = 'gs://gcp-cert-demo-1/taxi_mlp_trainer'\n",
    "\n",
    "\n",
    "# input training parameters\n",
    "training_params = {\n",
    "    'dense_neurons_1': 64,\n",
    "    'dense_neurons_2': 32,\n",
    "    'dense_neurons_3': 8,\n",
    "    'activation': 'relu',\n",
    "    'dropout_rate_1': 0.1,\n",
    "    'dropout_rate_2': 0.1,\n",
    "    'dropout_rate_3': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.1,\n",
    "    'chunk_size': 500000,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 1,\n",
    "    'validation_freq': 5,\n",
    "    'kernel_initial_1': 'normal',\n",
    "    'kernel_initial_2': 'normal',\n",
    "    'kernel_initial_3': 'normal',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mlp trainer\n",
    "mlp_trainer = train.MLPTrainer(\n",
    "    credentials=gcp_credentials,\n",
    "    project_name=gcp_vars['project_name'],\n",
    "    bucket=gcp_vars['bucket'],\n",
    "    job_id_prefix=job_id_prefix,\n",
    "    job_dir_prefix=job_dir_prefix,\n",
    "    table_id=gcp_vars['table_id'],\n",
    "    trainer_package_uri=gcp_vars['gcs_trainer_path']\n",
    ")\n",
    "\n",
    "# begin training job\n",
    "mlp_trainer.train(training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status of training job\n",
    "print(f'Training job status: {gcp.check_mle_job_status(mlp_trainer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy trained model\n",
    "model_name = 'taxi_cab_cash_or_credit'\n",
    "mlp_trainer.deploy(model_name, version_name='v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: basic predictions, need to flesh out\n",
    "# get test data\n",
    "rows = data.get_reader_rows(\n",
    "    credentials=gcp_credentials,\n",
    "    table_id=gcp_vars['table_id'],\n",
    "    partition_name='test'\n",
    ")\n",
    "vals = pd.DataFrame(list(rows)[:10]).drop(\n",
    "    'cash',\n",
    "    axis=1\n",
    ").values\n",
    "instances = []\n",
    "for instance in vals:\n",
    "    instances.append({'dense_input': list(instance)})\n",
    "\n",
    "# Todo - remove dependency on mlp_trainer module for model_dir\n",
    "# predict using deployed model\n",
    "predictor = predict.Predictor(\n",
    "    credentials=gcp_credentials,\n",
    "    project=gcp_vars['project_name'],\n",
    "    model=model_name\n",
    ")\n",
    "predictions = predictor.predict(instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gcpdemo1]",
   "language": "python",
   "name": "conda-env-gcpdemo1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
